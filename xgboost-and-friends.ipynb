{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/training_variants')\n",
    "trainingText = pd.read_csv('data/training_text', sep=\"\\|\\|\", engine='python', header=None, names=[\"ID\",\"Text\"], skiprows=1)\n",
    "\n",
    "test = pd.read_csv('data/test_variants')\n",
    "testText = pd.read_csv('data/test_text', sep=\"\\|\\|\", engine='python', header=None, names=[\"ID\",\"Text\"], skiprows=1)\n",
    "pid = test['ID'].values\n",
    "\n",
    "train = train.merge(trainingText, on='ID', how='left')\n",
    "test = test.merge(testText, on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def getTerms(sentences):\n",
    "    tokens = nltk.word_tokenize(sentences)\n",
    "    words = \" \".join([w.lower() for w in tokens if not w.lower()  in stop_words and  w.isalnum()])\n",
    "\n",
    "    return words\n",
    "trainingText['PText'] = trainingText['Text'].apply(getTerms)\n",
    "testText['PText'] = testText['Text'].apply(getTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingText.columns\n",
    "countVect = CountVectorizer()\n",
    "wordCountPerDoc = countVect.fit_transform(trainingText['PText'])\n",
    "wordCountPerDoc\n",
    "\n",
    "tTransformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(wordCountPerDoc)\n",
    "tfidf = tfidf.toarray()\n",
    "\n",
    "print(tfidf.shape)\n",
    "train['PText'] = trainingText['PText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magicFrequency(countVect, tfidf, wordCountPerDoc) : \n",
    "    su = wordCountPerDoc.sum(axis=0).tolist()[0]\n",
    "    sutf = tfidf.sum(axis=0).tolist()\n",
    "    \n",
    "    mostFrequentWords = [(word, su[idx], sutf[idx]) for word, idx in countVect.vocabulary_.items()]\n",
    "    mostFrequentWords = sorted(mostFrequentWords, key=lambda x : -x[1])\n",
    "    mostFrequentWordsTF = sorted(mostFrequentWords, key=lambda x : -x[2])\n",
    "    \n",
    "    return (mostFrequentWords,mostFrequentWordsTF)\n",
    "\n",
    "_, x = magicFrequency(countVect, tfidf, wordCountPerDoc)\n",
    "\n",
    "x[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizers = {}\n",
    "transformers = {}\n",
    "tfidfs = {}\n",
    "wordCounts = {}\n",
    "for c in train['Class'].unique():\n",
    "    print(c)\n",
    "    vectorizers[c] = CountVectorizer()\n",
    "    wordCounts[c] = vectorizers[c].fit_transform(train[train['Class']==c]['PText'])\n",
    "    \n",
    "    transformers[c] = TfidfTransformer(smooth_idf=False)\n",
    "    tfidfs[c] = transformers[c].fit_transform(wordCounts[c])\n",
    "    tfidfs[c] = tfidfs[c].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostPopularWords = {}\n",
    "\n",
    "for c in train['Class'].unique():\n",
    "    _, xx = magicFrequency(vectorizers[c], tfidfs[c],wordCounts[c])\n",
    "    mostPopularWords[c] = [x[0] for x in xx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequentWordsByText(text) :\n",
    "    cleanText = getTerms(text)\n",
    "    countVect = CountVectorizer()\n",
    "    wordCountPerDoc = countVect.fit_transform([cleanText])\n",
    "    \n",
    "    mostFrequentWords = [(word, wordCountPerDoc[0,idx]) for word, idx in countVect.vocabulary_.items()]\n",
    "\n",
    "    mostFrequentWords = sorted(mostFrequentWords, key=lambda x : -x[1])\n",
    "\n",
    "    return mostFrequentWords\n",
    "\n",
    "frequentWordsByText(train['PText'][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['fw'] = train['PText'].apply(lambda x : [wordTuple[0] for wordTuple in frequentWordsByText(x)])\n",
    "train['fw'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nboverlap = 250\n",
    "\n",
    "def overlapRange(fw, c, overlapSize):\n",
    "    n = len(list(set(fw[:overlapSize]) & set(mostPopularWords[c][:overlapSize])))\n",
    "    return float(n)/overlapSize\n",
    "    \n",
    "\n",
    "for i in range(1, 10):\n",
    "    columnName = 'overlap' + str(nboverlap) + 'C'+str(i)\n",
    "    train[columnName] = train['fw'].apply(lambda x: overlapRange(x, i, nboverlap))\n",
    "    print(columnName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mamourXgboost(df, features):\n",
    "    print('\\n##################\\nXGBoost\\n##################')\n",
    "    param = {}\n",
    "    # param['booster'] = 'gbtree'\n",
    "    #param['objective'] = 'binary:logistic'\n",
    "    # param[\"eval_metric\"] = \"error\"\n",
    "    # param['eta'] = 0.3\n",
    "    # param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['n_estimators'] = 200\n",
    "    param['learning_rate'] = 0.1\n",
    "    # param['min_child_weight'] = 1\n",
    "    # param['max_delta_step'] = 0\n",
    "    # param['subsample'] = 1\n",
    "    # param['colsample_bytree'] = 1\n",
    "    # param['silent'] = 1\n",
    "    # param['seed'] = 0\n",
    "    # param['base_score'] = 0.4\n",
    "\n",
    "    estimator = XGBClassifier()\n",
    "    estimator.set_params(**param)\n",
    "    x1, x2, y1, y2 = model_selection.train_test_split(df[features], df['Class'], test_size=0.2)\n",
    "#     calibratedCV = CalibratedClassifierCV(estimator, method='sigmoid', cv=5)\n",
    "#     calibratedCV.fit(x1, y1)\n",
    "#     prediction = calibratedCV.predict(x2)\n",
    "    \n",
    "#     print(calibratedCV.best_estimator_.booster())\n",
    "#     print(calibratedCV.calibrated_classifiers_)\n",
    "#     for c in calibratedCV.calibrated_classifiers_:\n",
    "#         print(c.feature_importances_)\n",
    "#         plot_importance(c)\n",
    "#         pyplot.show()\n",
    "#     feat_imp = pd.Series(calibratedCV.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title = 'Feature imp')\n",
    "    estimator.fit(x1, y1)\n",
    "    prediction = estimator.predict(x2)\n",
    "#     xgb.plot_importance(estimator, max_num_features = 50)\n",
    "    imp = estimator.feature_importances_\n",
    "    plot_importance(estimator)\n",
    "    pyplot.show()\n",
    "\n",
    "    print('accuracy %s %s' % (np.mean(prediction == y2), prediction[:5]))\n",
    "    #result = calibratedCV.predict(test)\n",
    "    # result = estimator.predict(test)\n",
    "    #return result\n",
    "    return imp\n",
    "\n",
    "geneDummies = pd.get_dummies(train['Gene'])\n",
    "\n",
    "features=['overlap100C'+str(c) for c in range(1, 10)]\n",
    "features.extend(geneDummies.columns)\n",
    "\n",
    "trainWithGenes = train.join(geneDummies)\n",
    "\n",
    "xgboostImp = mamourXgboost(trainWithGenes, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
